{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "I94 Immigration open ended Data is provided in the workspace to answer various key insights on US Immigration.\n",
    "Additional data files such as US Cities Demographics and Global temperature city wise also provided in the workspace.\n",
    "\n",
    "##### Objective\n",
    "\n",
    "Is to be provide anlaytics platform to enable data analyst for finding key insights. For this, need to create Analytics Database. The data to be populated by joining various input data pipeline through ETL tool.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "\n",
    "    * Project is to create etl data pipeline to ingest data from datafiles and transform to fit into data model for further storing the it in data warehouse.\n",
    "\n",
    "* Step 2: Explore and Assess the Data\n",
    "    * Pyspark is used create dataframe where data from files are extracted.\n",
    "\n",
    "    * Data Cleaning:\n",
    "\n",
    "    * EDA has been carried about in every dataframes to clean the datasets for missing values to drop the fields. \n",
    "    * Rows are deleted for missing values in essential fields and duplicate values.\n",
    "    * columns are also dropped for which missing values are more than 90%\n",
    "\n",
    "* Transformation:\n",
    "\n",
    "  * Date format is transformed from string values of I94 SAS data.\n",
    "  * From data dictionary file country code is mapped for country name.\n",
    "  * Aggregate city wise average temperature is calcualted.\n",
    "\n",
    "* Step 3: Define the Data Model\n",
    "\n",
    "* Data Model:\n",
    "  * Star Schema Data model created with referring the ER Diagram\n",
    "  * Primary keys identified in dimension table\n",
    "  * Fact table created and relation with dimention table is defined by join table.\n",
    "\n",
    "* Data Quality checks are done to confirm:\n",
    "\n",
    "  * No null value present in unique id of dataframe\n",
    "  * All values are unique in the key columns.\n",
    "  * Data is populated in dataframes.\n",
    "\n",
    "* Step 4: Run ETL to Model the Data\n",
    "\n",
    "\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "pd_imm = pd.read_csv('immigration_data_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_imm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.1 Load Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_imm.write.mode('overwrite').format('parquet').save(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm=spark.read.parquet(\"sas_data\")\n",
    "df_imm.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.1.a Explore Immigration Data for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in \"df_imm\" dataframe is: 3096313\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, cicid: string, i94yr: string, i94mon: string, i94cit: string, i94res: string, i94port: string, arrdate: string, i94mode: string, i94addr: string, depdate: string, i94bir: string, i94visa: string, count: string, dtadfile: string, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, biryear: string, dtaddto: string, gender: string, insnum: string, airline: string, admnum: string, fltno: string, visatype: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "print(f'Total rows in \"df_imm\" dataframe is: {df_imm.count()}')\n",
    "df_imm.printSchema() \n",
    "df_imm.head()\n",
    "df_imm.show(1)\n",
    "df_imm.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_counts_imm = df_imm.select([count(when(isnull(c), c)).alias(c) for c in df_imm.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_counts_imm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_row_imm = df_imm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "na_cols_imm = [c1 for c1 in cols_counts_imm.columns if cols_counts_imm.select(c1).first()[0] / total_row_imm >= 0.90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['occup', 'entdepu', 'insnum']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### 2.1.2 Cleaning Immigration Data for duplicates and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm_null_cols_dropped = df_imm.drop(*na_cols_imm)\n",
    "df_imm_null_cols_dropped.count()\n",
    "df_imm_null_cols_dropped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm_null_rows_dropped = df_imm_null_cols_dropped.dropna(subset=['cicid','arrdate','visatype'])\n",
    "df_imm_null_rows_dropped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm_cleaned = df_imm_null_rows_dropped.drop_duplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.2 Load US Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "df_demog = spark.read.csv('us-cities-demographics.csv', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   City| State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+-------+------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|Wichita|Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...| 8791|\n",
      "+-------+------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog.show(1)\n",
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              3|                3|               0|                13|          13|                    16|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write in parquet\n",
    "#df_demog.write.mode('overwrite').format('parquet').save(\"us-cities-demographics.parquet\")\n",
    "#df_demog = spark.read.format('parquet').load(\"us-cities-demographics.parquet\")\n",
    "cols_counts_demog = df_demog.select([count(when(isnull(c), c)).alias(c) for c in df_demog.columns])\n",
    "cols_counts_demog.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_row_demog = df_demog.count()\n",
    "total_row_demog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "na_cols_demog = [c1 for c1 in cols_counts_demog if cols_counts_demog.select(c1).first()[0] / total_row_demog >= 0.90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_demog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2875"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demog = df_demog.dropna(subset=['Male Population',\n",
    "        'Female Population',\n",
    "        'Number of Veterans',\n",
    "        'Foreign-born',\n",
    "        'Average Household Size'])\n",
    "\n",
    "df_demog= df_demog.drop_duplicates(subset=['City', 'State', 'State Code', 'Race'])\n",
    "\n",
    "df_demog.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.3 Load Airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport = spark.read.csv('airport-codes_csv.csv', sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|    type|             name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|heliport|Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.show(1)\n",
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_row_airport = df_airport.count()\n",
    "total_row_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_counts_airport = df_airport.select([count(when(isnull(c), c)).alias(c) for c in df_airport.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "na_cols_airport = [c1 for c1 in cols_counts_airport.columns if cols_counts_airport.select(c1).first()[0] / total_row_airport >= 0.90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ident', 0.0), ('type', 0.0), ('name', 0.0), ('elevation_ft', 0.127208352246936), ('continent', 0.0), ('iso_country', 0.0), ('iso_region', 0.0), ('municipality', 0.10305946436677259), ('gps_code', 0.25501588742623693), ('iata_code', 0.8331547889241943), ('local_code', 0.4791466182478438), ('coordinates', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print([(c1, cols_counts_airport.select(c1).first()[0] / total_row_airport) for c1 in cols_counts_airport.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_null_cols_dropped = df_airport.drop(*na_cols_airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_missing_rows_dropped = df_airport_null_cols_dropped.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_cleaned = df_airport_missing_rows_dropped.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.4 Load US Global Temperature by city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp.write.partitionBy('Country').mode('overwrite').parquet('GlobalLandTemperaturesByCity.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp = spark.read.format('parquet').load('GlobalLandTemperaturesByCity.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------------------+------+--------+---------+-------+\n",
      "|        dt|  AverageTemperature|AverageTemperatureUncertainty|  City|Latitude|Longitude|Country|\n",
      "+----------+--------------------+-----------------------------+------+--------+---------+-------+\n",
      "|1840-01-01|              -2.025|                        2.537| Taian|  36.17N|  117.35E|  China|\n",
      "|1820-08-01|              20.146|                        2.286|Ürümqi|  44.20N|   87.20E|  China|\n",
      "|1840-02-01|-0.20300000000000007|                        2.465| Taian|  36.17N|  117.35E|  China|\n",
      "|1820-09-01|              15.331|                        1.775|Ürümqi|  44.20N|   87.20E|  China|\n",
      "|1840-03-01|                4.82|           2.0180000000000002| Taian|  36.17N|  117.35E|  China|\n",
      "+----------+--------------------+-----------------------------+------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show(5)\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+-------+\n",
      "|        dt|  AverageTemperature|  City|Country|\n",
      "+----------+--------------------+------+-------+\n",
      "|1840-01-01|              -2.025| Taian|  China|\n",
      "|1820-08-01|              20.146|Ürümqi|  China|\n",
      "|1840-02-01|-0.20300000000000007| Taian|  China|\n",
      "|1820-09-01|              15.331|Ürümqi|  China|\n",
      "|1840-03-01|                4.82| Taian|  China|\n",
      "+----------+--------------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = df_temp.drop('AverageTemperatureUncertainty', 'Latitude', 'Longitude')\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_row_temp = df_temp.count()\n",
    "total_row_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----+-------+\n",
      "| dt|AverageTemperature|City|Country|\n",
      "+---+------------------+----+-------+\n",
      "|  0|            364130|   0|      0|\n",
      "+---+------------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_counts_temp = df_temp.select([count(when(isnull(c), c)).alias(c) for c in df_temp.columns])\n",
    "cols_counts_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "na_cols_temp = [c1 for c1 in cols_counts_temp.columns if cols_counts_temp.select(c1).first()[0] / total_row_temp >= 0.90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_temp # no columns have null values more than 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8235082"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df_temp.dropna(subset=['AverageTemperature'])\n",
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_cleaned = df_temp.drop_duplicates(subset=['dt', 'City', 'Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8190783"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "![Data Model](ImmigrationERD.png)\n",
    "**Map of the conceptual data model** \n",
    "\n",
    "##### For the analytics Star-schema has been chosen which is composed of 4 Dimension tables and corresponding Fact table created as follow:\n",
    "\n",
    "1. Immigration Calendar Dimension Table:\n",
    "Primary key is chosen as arrdate which is extracted from immigration dataset.\n",
    "Further from arrdata year, month, day, week and weekday have been extracted.\n",
    "\n",
    "2. Country_temperature Dimension Table:\n",
    "From Global Temperature City wise dataset, this dimension table is created to map country and average temperature.\n",
    "Also from US demographic dataset the table is getting country code.\n",
    "\n",
    "\n",
    "3. US Demographic Dimension Table:\n",
    "The table is created from US Demographic dataset to find out various key sight on immigration.\n",
    "The table enables the analyst for taking data driven decision from various joins statements.\n",
    "\n",
    "4. Visa Type Dimension Table:\n",
    "As required by analyst for decision to be taken based on visa type, this table is created from immigration dataset.\n",
    "\n",
    "Immigration Fact Table: \n",
    "The centre table which joins other dimension table as M:1 mapping relation with common keys.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. The datasets are loaded into spark cluster into dataframe.\n",
    "2. SAS fromat of Immigration dataset is converted to parquet. Also, the GlobalLandTemperatureDataSet downloaded from Kaggle is also converted to parquet format with partition by country.\n",
    "3. All the dataframes are cleaned as per following strategy:\n",
    "\n",
    "    * The columns are dropped where more than 90% values are missing.\n",
    "    \n",
    "    * The columns are dropped which are not required for this analysis.\n",
    "    \n",
    "    * The rows are dropped where particular columns are having no values.\n",
    "    \n",
    "    * The rows which are having duplicate values in set of columns are dropped\n",
    "    \n",
    "    \n",
    " 4. Table creations as per the data model and loading of data from dataframes:\n",
    " \n",
    "    * Created `US immigration calendar dimension table` and data is extracted and loaded from `cleaned immigration dataframe`\n",
    "    \n",
    "    * Created  `Country_temperature Dimension Table` and data is loaded by joining Global temperature dataframe and US Demographic dataframe\n",
    "    \n",
    "    * Created `US Demographic Dimension table` and data is extracted and loaded from respective dataframe\n",
    "    \n",
    "    * Created `visa type dimension table` and data is extracted and loaded from cleaned `immigration dataframe`\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dataframe for immigration calendar table consisting all distinct arrival dates from immigration dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+-------------+------------+---------------+\n",
      "|   arrdate|arrival_day|arrival_week|arrival_month|arrival_year|arrival_weekday|\n",
      "+----------+-----------+------------+-------------+------------+---------------+\n",
      "|2016-04-22|         22|          16|            4|        2016|              6|\n",
      "|2016-04-15|         15|          15|            4|        2016|              6|\n",
      "|2016-04-18|         18|          16|            4|        2016|              2|\n",
      "|2016-04-09|          9|          14|            4|        2016|              7|\n",
      "|2016-04-11|         11|          15|            4|        2016|              2|\n",
      "+----------+-----------+------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_imm_cleaned.select(\"arrdate\").show(5)\n",
    "conv_SAS_date = udf(lambda SAS_date: (datetime(1960, 1, 1).date() + timedelta(SAS_date)).isoformat())\n",
    "df_imm_cal = df_imm_cleaned. \\\n",
    "    select(df_imm_cleaned['arrdate']). \\\n",
    "    withColumn('arrdate', conv_SAS_date(df_imm_cleaned.arrdate)). \\\n",
    "    distinct().\\\n",
    "    withColumn('arrival_day', dayofmonth('arrdate')).\\\n",
    "    withColumn('arrival_week', weekofyear('arrdate')).\\\n",
    "    withColumn('arrival_month', month('arrdate')).\\\n",
    "    withColumn('arrival_year', year('arrdate')).\\\n",
    "    withColumn('arrival_weekday', dayofweek('arrdate'))\n",
    "\n",
    "df_imm_cal.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm_cal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dataframe for visa type table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- visa_type_key: long (nullable = false)\n",
      "\n",
      "+---------+-------------+\n",
      "|visa_type|visa_type_key|\n",
      "+---------+-------------+\n",
      "|       F2| 103079215104|\n",
      "|      GMB| 352187318272|\n",
      "|       B2| 369367187456|\n",
      "|       F1| 498216206336|\n",
      "|      CPL| 601295421440|\n",
      "|       I1| 704374636544|\n",
      "|       WB| 738734374912|\n",
      "|       M1| 747324309504|\n",
      "|       B1| 807453851648|\n",
      "|       WT| 884763262976|\n",
      "|       M2|1151051235328|\n",
      "|       CP|1314259992576|\n",
      "|      GMT|1331439861760|\n",
      "|       E1|1348619730944|\n",
      "|        I|1391569403904|\n",
      "|       E2|1554778161152|\n",
      "|      SBP|1709396983808|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visa = df_imm_cleaned.\\\n",
    "    select('visatype').\\\n",
    "    distinct().\\\n",
    "    withColumn('visa_type_key', monotonically_increasing_id()).\\\n",
    "    withColumnRenamed('visatype', 'visa_type')\n",
    "df_visa.printSchema()\n",
    "df_visa.show()\n",
    "#print(df_visa.distinct().count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create Country_temperature dimention table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|country_code|        country_name|\n",
      "+------------+--------------------+\n",
      "|       582.0|Mexico air sea, a...|\n",
      "|       236.0|         Afghanistan|\n",
      "|       101.0|             Albania|\n",
      "|       316.0|             Algeria|\n",
      "|       102.0|             Andorra|\n",
      "|       324.0|              Angola|\n",
      "|       529.0|            Anguilla|\n",
      "|       518.0|     Antigua-barbuda|\n",
      "|       687.0|          Argentina |\n",
      "|       151.0|             Armenia|\n",
      "|       532.0|               Aruba|\n",
      "|       438.0|           Australia|\n",
      "|       103.0|             Austria|\n",
      "|       152.0|          Azerbaijan|\n",
      "|       512.0|             Bahamas|\n",
      "|       298.0|             Bahrain|\n",
      "|       274.0|          Bangladesh|\n",
      "|       513.0|            Barbados|\n",
      "|       104.0|             Belgium|\n",
      "|       581.0|              Belize|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- country_code: double (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_list = []\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    lines = f.readlines()\n",
    "    #print(len(lines))\n",
    "    for line in lines:\n",
    "        #print(type(line))\n",
    "        country_code,country_name = line.split('=')\n",
    "        #print(country_code.strip(), country_name.strip()[1:-1])\n",
    "        country_name_final = country_name.strip()[1:-1].capitalize()\n",
    "        #print(dict_country[country_name_final])\n",
    "        dict_country = dict(country_code=float(country_code.strip()), country_name=country_name_final)\n",
    "        country_list.append(dict_country)\n",
    "\n",
    "#print(country_list)       \n",
    "# creating a dataframe\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    # code that produces a warning\n",
    "    country_code_df = spark.createDataFrame(country_list)\n",
    "    \n",
    "country_code_df.show()\n",
    "country_code_df.printSchema()\n",
    "#country_code_df.select('country_code').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|country_name|average_temperature|\n",
      "+------------+-------------------+\n",
      "|        Chad| 27.189829394812683|\n",
      "|      Russia| 3.3472679828734857|\n",
      "|    Paraguay| 22.784014312977153|\n",
      "|       Yemen| 25.768407664453854|\n",
      "|     Senegal|  25.98417669449079|\n",
      "|      Sweden|  5.665518003790279|\n",
      "|      Guyana|  26.54984937439856|\n",
      "| Philippines|   26.5164624674648|\n",
      "|       Burma| 26.016839989290045|\n",
      "|     Eritrea| 24.001515877771144|\n",
      "|    Djibouti| 29.152790108564506|\n",
      "|    Malaysia|  26.43475662438397|\n",
      "|   Singapore| 26.523102826510677|\n",
      "|      Turkey| 12.951888167466578|\n",
      "|      Malawi| 21.347872026498056|\n",
      "|        Iraq| 19.884738137449155|\n",
      "|     Germany|  8.482790790264009|\n",
      "| Afghanistan| 13.816496896263578|\n",
      "|    Cambodia| 26.918136297728335|\n",
      "|      Jordan| 18.360980886539238|\n",
      "+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  #dt|AverageTemperature| City|Country\n",
    "conv_avg_temp_float = udf(lambda x : float(x))\n",
    "#get_country_code = udf(lambda x : )    \n",
    "df_country_temp = df_temp.\\\n",
    "    groupBy(col('country').alias('country_name')).\\\n",
    "    agg(avg(conv_avg_temp_float(col('AverageTemperature'))).alias('average_temperature')) #.\\\n",
    "    #withColumn('country_code', get_country_code())\n",
    "#df_country_temp = df_country_temp.withColumn('country_code', dict_country.get(df_country_temp.country_name))  \n",
    "    \n",
    "df_country_temp.show()\n",
    "#df_country_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_country_temp = df_country_temp.join(country_code_df, df_country_temp.country_name == country_code_df.country_name).\\\n",
    "                    select(country_code_df.country_code, df_country_temp.country_name, df_country_temp.average_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------------+\n",
      "|country_code|country_name|average_temperature|\n",
      "+------------+------------+-------------------+\n",
      "|       384.0|        Chad| 27.189829394812683|\n",
      "|       693.0|    Paraguay| 22.784014312977153|\n",
      "|       158.0|      Russia| 3.3472679828734857|\n",
      "|       216.0|       Yemen| 25.768407664453854|\n",
      "|       391.0|     Senegal|  25.98417669449079|\n",
      "|       130.0|      Sweden|  5.665518003790279|\n",
      "|       603.0|      Guyana|  26.54984937439856|\n",
      "|       243.0|       Burma| 26.016839989290045|\n",
      "|       372.0|     Eritrea| 24.001515877771144|\n",
      "|       260.0| Philippines|   26.5164624674648|\n",
      "|       322.0|    Djibouti| 29.152790108564506|\n",
      "|       273.0|    Malaysia|  26.43475662438397|\n",
      "|       207.0|   Singapore| 26.523102826510677|\n",
      "|       264.0|      Turkey| 12.951888167466578|\n",
      "|       345.0|      Malawi| 21.347872026498056|\n",
      "|       250.0|        Iraq| 19.884738137449155|\n",
      "|       112.0|     Germany|  8.482790790264009|\n",
      "|       236.0| Afghanistan| 13.816496896263578|\n",
      "|       253.0|      Jordan| 18.360980886539238|\n",
      "|       376.0|      Rwanda| 19.077631823461093|\n",
      "+------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_country_temp = df_country_temp.withColumn('country_code', df_country_temp.country_code.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_temp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create US Demographic table dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+---+\n",
      "|        City|     State|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                Race|Count| id|\n",
      "+------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+---+\n",
      "|Arden-Arcade|California|      41.5|          47596|            48680|           96276|              6511|       13458|                  2.18|        CA|Black or African-...|13647|  0|\n",
      "| Bloomington| Minnesota|      40.9|          43318|            43118|           86436|              6176|       10728|                   2.3|        MN|Black or African-...| 5828|  1|\n",
      "+------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog = df_demog.withColumnRenamed('Median Age','median_age') \\\n",
    "            .withColumnRenamed('Male Population', 'male_population') \\\n",
    "            .withColumnRenamed('Female Population', 'female_population') \\\n",
    "            .withColumnRenamed('Total Population', 'total_population') \\\n",
    "            .withColumnRenamed('Number of Veterans', 'number_of_veterans') \\\n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born') \\\n",
    "            .withColumnRenamed('Average Household Size', 'average_household_size') \\\n",
    "            .withColumnRenamed('State Code', 'state_code')\n",
    "df_demog = df_demog.withColumn('id', monotonically_increasing_id())\n",
    "df_demog.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create fact table f_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+----------+-------+----------+-------+----------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+---------+-------------+------------+------------+-------------------+\n",
      "|record_id| i94yr|i94mon|i94cit|country_id|i94port|   arrdate|i94mode|state_code|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|      admnum|fltno|visatype|visa_type|visa_type_key|country_code|country_name|average_temperature|\n",
      "+---------+------+------+------+----------+-------+----------+-------+----------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+---------+-------------+------------+------------+-------------------+\n",
      "|5761447.0|2016.0|   4.0| 299.0|     299.0|    LOS|2016-04-30|    1.0|        CA|20603.0|  44.0|    2.0|  1.0|20160430|    null| null|      T|      O|   null|      M| 1972.0|10292016|     F|  null|     KE|9.04497085E8|   17|      B2|       B2| 369367187456|         299|    Mongolia|-3.3654853195164023|\n",
      "|5761448.0|2016.0|   4.0| 299.0|     299.0|    CHI|2016-04-30|    1.0|        IL|20620.0|  42.0|    2.0|  1.0|20160430|    null| null|      T|      O|   null|      M| 1974.0|10292016|     F|  null|     SK|9.07683485E8|  945|      B2|       B2| 369367187456|         299|    Mongolia|-3.3654853195164023|\n",
      "+---------+------+------+------+----------+-------+----------+-------+----------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+---------+-------------+------------+------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact =   df_imm.withColumnRenamed('cicid','record_id') \\\n",
    "            .withColumnRenamed('i94res', 'country_id') \\\n",
    "            .withColumnRenamed('i94addr', 'state_code') \n",
    "df_fact = df_fact.join(df_visa, df_visa.visa_type == df_imm.visatype)\n",
    "\n",
    "get_datetime = udf(lambda x: (datetime(1960, 1, 1).date() + timedelta(x)).isoformat())\n",
    "df_fact = df_fact.withColumn(\"arrdate\", get_datetime(df_fact.arrdate))\n",
    "df_fact = df_fact.join(df_country_temp, df_country_temp.country_code == df_fact.country_id)\n",
    "\n",
    "\n",
    "#df_fact.drop('visatype', 'visa_type', 'country_name', 'average_temperature')\n",
    "df_fact.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- record_id: integer (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- country_id: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- visa_type_key: integer (nullable = false)\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      "\n",
      "+---------+------+------+------+----------+-------+----------+-------+----------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+---------+-------------+------------+------------+-------------------+\n",
      "|record_id| i94yr|i94mon|i94cit|country_id|i94port|   arrdate|i94mode|state_code|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|visa_type|visa_type_key|country_code|country_name|average_temperature|\n",
      "+---------+------+------+------+----------+-------+----------+-------+----------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+---------+-------------+------------+------------+-------------------+\n",
      "|  5761447|2016.0|   4.0| 299.0|       299|    LOS|2016-04-30|    1.0|        CA|20603.0|  44.0|    2.0|  1.0|20160430|    null| null|      T|      O|   null|      M| 1972.0|10292016|     F|  null|     KE|  9.04497085E8|   17|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761448|2016.0|   4.0| 299.0|       299|    CHI|2016-04-30|    1.0|        IL|20620.0|  42.0|    2.0|  1.0|20160430|    null| null|      T|      O|   null|      M| 1974.0|10292016|     F|  null|     SK|  9.07683485E8|  945|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761451|2016.0|   4.0| 299.0|       299|    WAS|2016-04-30|    1.0|        DC|   null|  34.0|    3.0|  1.0|20160430|     ULN| null|      G|   null|   null|   null| 1982.0|     D/S|     M|  null|     UA|9.501061723E10|02108|      F2|       F2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761452|2016.0|   4.0| 299.0|       299|    WAS|2016-04-30|    1.0|        DC|20593.0|  26.0|    2.0|  1.0|20160430|     ULN| null|      G|      N|   null|      M| 1990.0|10292016|     F|  null|     UA|9.501043203E10|02108|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761453|2016.0|   4.0| 299.0|       299|    WAS|2016-04-30|    1.0|        DC|20580.0|  50.0|    1.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1966.0|10292016|     M|  null|     KE|9.496416333E10|00093|      B1|       B1|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761454|2016.0|   4.0| 299.0|       299|    WAS|2016-04-30|    1.0|        DC|20665.0|  66.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     KE|9.496506083E10|00093|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761455|2016.0|   4.0| 299.0|       299|    WAS|2016-04-30|    1.0|        VA|20582.0|  46.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1970.0|10292016|     F|  null|     KE|9.496722313E10|00093|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761458|2016.0|   4.0| 299.0|       299|    WAS|2016-04-30|    1.0|        WA|20588.0|  40.0|    1.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1976.0|10292016|     M|  null|     KE|9.496448503E10|00093|      B1|       B1|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761459|2016.0|   4.0| 299.0|       299|    LOS|2016-04-30|    1.0|        CA|   null|  27.0|    1.0|  1.0|20160430|     ULN| null|      G|   null|   null|   null| 1989.0|04292018|     F|  null|     KE|9.497572523E10|00017|      E2|       E2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761460|2016.0|   4.0| 299.0|       299|    LOS|2016-04-30|    1.0|        CA|20691.0|  20.0|    2.0|  1.0|20160430|     ULN| null|      G|      N|   null|      M| 1996.0|10292016|     F|  null|     KE|9.501202163E10|00011|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761461|2016.0|   4.0| 299.0|       299|    LOS|2016-04-30|    1.0|        CA|20589.0|  61.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1955.0|10292016|     M|  null|     KE|9.497428333E10|00017|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761462|2016.0|   4.0| 299.0|       299|    LOS|2016-04-30|    1.0|        CA|20589.0|  59.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1957.0|10292016|     F|  null|     KE|9.497411103E10|00017|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761463|2016.0|   4.0| 299.0|       299|    LOS|2016-04-30|    1.0|        CA|20603.0|  75.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1941.0|10292016|     F|  null|     KE|9.497431453E10|00017|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761464|2016.0|   4.0| 299.0|       299|    CHI|2016-04-30|    1.0|        KY|20592.0|  44.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1972.0|10292016|     M|  null|     KE|9.496450523E10|00037|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761465|2016.0|   4.0| 299.0|       299|    CHI|2016-04-30|    1.0|        KY|20592.0|  41.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     KE|9.496467493E10|00037|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761466|2016.0|   4.0| 299.0|       299|    CHI|2016-04-30|    1.0|        KY|20595.0|  47.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1969.0|10292016|     M|  null|     KE|9.496498743E10|00037|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761467|2016.0|   4.0| 299.0|       299|    SFR|2016-04-30|    1.0|        CA|20581.0|  31.0|    1.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1985.0|10292016|     F|  null|     UA|9.498117943E10|00892|      B1|       B1|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761468|2016.0|   4.0| 299.0|       299|    SFR|2016-04-30|    1.0|        CA|20653.0|  59.0|    2.0|  1.0|20160430|     ULN| null|      T|      N|   null|      M| 1957.0|10292016|     F|  null|     KE|  9.05775985E8|   23|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761469|2016.0|   4.0| 299.0|       299|    SFR|2016-04-30|    1.0|        CO|   null|  27.0|    3.0|  1.0|20160430|     ULN| null|      G|   null|   null|   null| 1989.0|     D/S|     F|  null|     UA|9.498313923E10|00892|      F1|       F1|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "|  5761470|2016.0|   4.0| 299.0|       299|    SFR|2016-04-30|    1.0|        CO|20602.0|  30.0|    2.0|  1.0|20160430|     ULN| null|      G|      O|   null|      M| 1986.0|10292016|     F|  null|     UA|9.498236923E10|00892|      B2|       B2|            0|         299|    Mongolia|-3.3654853195164023|\n",
      "+---------+------+------+------+----------+-------+----------+-------+----------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+---------+-------------+------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact = df_fact.withColumn('country_id', df_fact.country_id.cast(IntegerType()))\n",
    "df_fact = df_fact.withColumn('visa_type_key', df_fact.visa_type_key.cast(IntegerType()))\n",
    "df_fact = df_fact.withColumn('record_id', df_fact.record_id.cast(IntegerType()))\n",
    "df_fact = df_fact.withColumn('arrdate', df_fact.arrdate.cast(DateType()))\n",
    "df_fact.printSchema()\n",
    "df_fact.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "dict_tables = {\n",
    "    'f_immigration': df_fact,\n",
    "    'd_visa_type': df_visa,\n",
    "    'd_immigration_calendar': df_imm_cal,\n",
    "    'd_usa_demographics': df_demog,\n",
    "    'd_country_temp': df_country_temp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: f_immigration has 1976407 records.\n",
      "Passed: d_visa_type has 17 records.\n",
      "Passed: d_immigration_calendar has 30 records.\n",
      "Passed: d_usa_demographics has 2875 records.\n",
      "Passed: d_country_temp has 131 records.\n"
     ]
    }
   ],
   "source": [
    "# Check if data is available in dataframes\n",
    "for table, df in dict_tables.items():\n",
    "    # quality check for table\n",
    "    total_count = df.count()\n",
    "    if total_count == 0:\n",
    "        print(f\"Failed: {table} has zero records!\")\n",
    "    else:\n",
    "        print(f\"Passed: {table} has {total_count} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: f_immigration.record_id contains unique values\n",
      "Passed: d_visa_type.visa_type contains unique values\n",
      "Passed: d_immigration_calendar.arrdate contains unique values\n",
      "Passed: d_country_temp.country_code contains unique values\n"
     ]
    }
   ],
   "source": [
    "# Check if 'id' columns conatains unique values\n",
    "for table, df in dict_tables.items():\n",
    "    #print(df.columns)\n",
    "    # quality check for table\n",
    "    unique_row_count = int(df.select(countDistinct(df.columns[0])).first()[0])\n",
    "    total_count = df.count()\n",
    "    #print(f'{unique_row_count}:: {total_count}')\n",
    "    if table == 'd_usa_demographics': continue\n",
    "    if unique_row_count == total_count:\n",
    "        print(f\"Passed: {table}.{df.columns[0]} contains unique values\")\n",
    "    else: \n",
    "        print(f\"Failed: {table}.{df.columns[0]} contains duplicate values in unique id column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Passed: record_id unique id doesn't contain null values\n",
      "0\n",
      "Passed: visa_type unique id doesn't contain null values\n",
      "0\n",
      "Passed: arrdate unique id doesn't contain null values\n",
      "0\n",
      "Passed: City unique id doesn't contain null values\n",
      "0\n",
      "Passed: country_code unique id doesn't contain null values\n"
     ]
    }
   ],
   "source": [
    "# Check if 'id' columns contains any null value\n",
    "def check_null_id(df):\n",
    "    null_count = df.filter(col(df.columns[0]) == '').count()\n",
    "    #print(null_count)\n",
    "    if null_count == 0:\n",
    "        print(f\"Passed: {df.columns[0]} unique id doesn't contain null values\")\n",
    "    else: (f\"Failed: {df.columns[0]} unique id contains null value\")\n",
    "\n",
    "for table, df in dict_tables.items():\n",
    "    check_null_id(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Fact Table\n",
    "|Feature|Description|\n",
    "|-------|-----------|\n",
    "record_id|Unique record ID\n",
    "country_residence_code|3 digit code for immigrant country of residence\n",
    "visa_type_key|A numerical key that links to the visa_type dimension table\n",
    "state_code|US state of arrival\n",
    "i94yr|4 digit year\n",
    "i94mon|Numeric month\n",
    "i94port|Port of admission\n",
    "arrdate|Arrival Date in the USA\n",
    "i94mode|Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "i94addr|USA State of arrival\n",
    "depdate|Departure Date from the USA\n",
    "i94bir|Age of Respondent in Years\n",
    "i94visa|Visa codes collapsed into three categories\n",
    "count|Field used for summary statistics\n",
    "dtadfile|Character Date Field - Date added to I-94 Files\n",
    "visapost|Department of State where where Visa was issued\n",
    "occup|Occupation that will be performed in U.S\n",
    "entdepa|Arrival Flag - admitted or paroled into the U.S.\n",
    "entdepd|Departure Flag - Departed, lost I-94 or is deceased\n",
    "entdepu|Update Flag - Either apprehended, overstayed, adjusted to perm residence\n",
    "matflag|Match flag - Match of arrival and departure records\n",
    "biryear|4 digit year of birth\n",
    "dtaddto|Character Date Field - Date to which admitted to U.S. (allowed to stay until)\n",
    "gender|Non-immigrant sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Country Dimension Table\n",
    "* The `country_code` and `country_name` fields come from the labels description SAS file  \n",
    "* the `average_temperature` data comes from the `GlobalLandTemperatureByCities` file.\n",
    "\n",
    "Feature|Description\n",
    "---------|----------\n",
    "country_code|Unique country code\n",
    "country_name|Name of country\n",
    "average_temperature|Average temperature of country\n",
    "\n",
    "##### Visa Type Dimension Table\n",
    "* `visa_type` is taken from SAS files\n",
    "* `visa_type_key` is generated column\n",
    "\n",
    "Feature|Description\n",
    "---------|----------\n",
    "visa_type_key|Unique id for each visa issued\n",
    "visa_type|Name of visa\n",
    "\n",
    "##### Immigration Calendar Dimension Table\n",
    "* The `arrdate` from the immigration dataset is extracted and trnasformed to date type.\n",
    "* Also other columns are extracted from `arrdate` field\n",
    "\n",
    "Feature|Description\n",
    "---------|----------\n",
    "arrdate|Arrival date into US\n",
    "arrival_year|Arrival year into US\n",
    "arrival_month|Arrival Month\n",
    "arrival_day|Arrival Day\n",
    "arrival_week|Arrival Week\n",
    "arrival_weekday|Arrival WeekDay\n",
    "\n",
    "##### US Demographics Dimension Table - data dictionary\n",
    "This dataset is extracted from the `us-cities-demographics` file.\n",
    "\n",
    "Feature|Description\n",
    "---------|----------\n",
    "id|Record id\n",
    "state_code|US state code\n",
    "City|City Name\n",
    "State|US State where city is located\n",
    "Median Age|Median age of the population\n",
    "Male Population|Count of male population\n",
    "Female Population|Count of female population\n",
    "Total Population|Count of total population\n",
    "Number of Veterans|Count of total Veterans\n",
    "Foreign born|Count of residents of the city that were not born in the city\n",
    "Average Household Size|Average city household size\n",
    "Race|Respondent race\n",
    "Count|Count of city's individual per race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Rationale for the choice of tools and technologies for the project\n",
    "\n",
    "* Apache spark is selected for this project to develop an ETL tool which has following features:\n",
    "\n",
    "* Spark enables to extract from various file format including SAS format using config jar files.\n",
    "* Apache Spark processes dataframes in-memory enables to develop lightning-fast ETL tools for analytics purpose.\n",
    "* Spark has vast APIs to work on big data for transformation of datatype to creating SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. US Immigration data is updated monthly hence the data should be updated monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. Using spark we can develop scalable solution, hence Spark can handle data even if data volume increased by 100x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. For data population of a dashboard that must be updated on a daily basis by 7am every day, the data pipeline (ETL) can be scheduled using Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "5. For multiuser access of data, data should be loaded in a Datawarehouse eg. Amazon Redshift from ETL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
